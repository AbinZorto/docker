version: "3.8"
services:
  mineru-api:
    build: .
    ports:
      - "8000:8000"
      - "8001:8001" # Add this for Slang server
    environment:
      # Your existing environment variables (preserved)
      - MINERU_API_KEY=${MINERU_API_KEY:-cbe9f548-54fe-4e9c-ada6-15291466d213}
      - REDIS_URL=redis://redis:6379
      - MAX_FILE_SIZE=100
      - CACHE_TTL=7200
      - DEBUG=false
      - DOMAIN=${DOMAIN:-mineru.writemine.com}
      - SLANG_PORT=8001
      - SLANG_HOST=0.0.0.0

      # NEW: GPU-specific environment variables
      - MINERU_DEVICE=cuda
      - MINERU_BACKEND=vlm-sglang-client
      - NVIDIA_VISIBLE_DEVICES=all
      - MODEL_CACHE_DIR=/app/models
    volumes:
      # Your existing volumes (preserved)
      - ./uploads:/app/uploads
      - ./outputs:/app/outputs
      - ./cache:/app/cache

      # NEW: Model storage volume for GPU models
      - mineru_models:/app/models
      # CRITICAL: HuggingFace cache persistence (this fixes the 10-minute download issue)
      - hf_cache:/root/.cache/huggingface
      # Optional: PyTorch compilation cache
      - torch_cache:/root/.cache/torch
    depends_on:
      - redis
    restart: unless-stopped

    # Your existing healthcheck (preserved)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # NEW: GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  redis:
    image: redis:7.2-alpine # Your version preserved
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

    # Redis uses system RAM (not VRAM), so we can be more generous
    # T4 has 16GB VRAM (separate) + 16GB system RAM
    # Allocating 4GB for Redis cache leaves 12GB system RAM for MinerU app/files
    command: redis-server --appendonly yes --maxmemory 4gb --maxmemory-policy allkeys-lru

volumes:
  redis_data: # Your existing volume
  mineru_models: # NEW: For GPU model storage
  hf_cache: # CRITICAL: HuggingFace model cache persistence
  torch_cache: # Optional: PyTorch compilation cache
